"""
Tarife, kontÃ¶r ve paket iÅŸlemleri iÃ§in Ã¶zel agent
"""

from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate


class TarifeAgent:
    """Tarife, kontÃ¶r ve paket iÅŸlemleri iÃ§in Ã¶zel agent sÄ±nÄ±fÄ±"""
    
    def __init__(self, model_name: str = "gemma3:latest"):
        """
        Tarife Agent'i baÅŸlatÄ±r
        
        Args:
            model_name: Ollama'da kullanÄ±lacak model adÄ±
        """
        self.llm = OllamaLLM(
            model=model_name,
            base_url="http://localhost:11434"
        )
        
        # Tarife ve paket konularÄ±na Ã¶zel prompt
        self.prompt = ChatPromptTemplate.from_template(
            """Sen TelekomTÃ¼rk'Ã¼n tarife ve paket departmanÄ±ndan bir uzmansÄ±n. 
            MÃ¼ÅŸterilerin tarife deÄŸiÅŸikliÄŸi, paket seÃ§imi, kontÃ¶r iÅŸlemleri konularÄ±nda yardÄ±mcÄ± oluyorsun.
            
            Hizmetlerin:
            - Tarife seÃ§imi ve deÄŸiÅŸikliÄŸi
            - Ä°nternet paketleri (Fiber, ADSL, Mobil)
            - KonuÅŸma dakikalarÄ± ve SMS paketleri
            - KontÃ¶r yÃ¼kleme iÅŸlemleri
            - Kampanya ve indirimler
            - Paket kullanÄ±m sorgularÄ±
            - Hat aÃ§ma/kapatma iÅŸlemleri
            
            MÃ¼ÅŸteriye ihtiyaÃ§larÄ±na uygun paketleri Ã¶ner.
            Fiyat ve Ã¶zellik karÅŸÄ±laÅŸtÄ±rmalarÄ± yap.
            AdÄ±m adÄ±m paket deÄŸiÅŸim sÃ¼recini anlat.
            
            MÃ¼ÅŸteri talebi: {user_input}
            
            Tarife UzmanÄ± YanÄ±tÄ±:"""
        )
    
    def handle_tarife_request(self, user_input: str) -> str:
        """
        Tarife ve paket talebini iÅŸler
        
        Args:
            user_input: MÃ¼ÅŸterinin tarife/paket talebi
            
        Returns:
            Tarife uzmanÄ±nÄ±n yanÄ±tÄ±
        """
        print(f"ğŸ“¦ Tarife ve Paket DepartmanÄ±: Talep iÅŸleniyor...")
        
        # Prompt'u hazÄ±rla ve LLM'e gÃ¶nder
        formatted_prompt = self.prompt.format(user_input=user_input)
        response = self.llm.invoke(formatted_prompt)
        
        return response
